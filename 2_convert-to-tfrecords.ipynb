{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import argparse\n",
    "import io\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "SCRIPT_PATH = os.path.dirname(os.path.abspath('./hangul-WR'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default data paths.\n",
    "DEFAULT_LABEL_CSV = os.path.join(SCRIPT_PATH, './image-data/labels-map.csv')\n",
    "DEFAULT_LABEL_FILE = os.path.join(SCRIPT_PATH,\n",
    "                                  './labels/2350-common-hangul.txt')\n",
    "DEFAULT_OUTPUT_DIR = os.path.join(SCRIPT_PATH, './tfrecords-output')\n",
    "DEFAULT_NUM_SHARDS_TRAIN = 3\n",
    "DEFAULT_NUM_SHARDS_TEST = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFRecordsConverter(object):\n",
    "    \"\"\"Class that handles converting images to TFRecords.\"\"\"\n",
    "\n",
    "    def __init__(self, labels_csv, label_file, output_dir,\n",
    "                 num_shards_train, num_shards_test):\n",
    "\n",
    "        self.output_dir = output_dir\n",
    "        self.num_shards_train = num_shards_train\n",
    "        self.num_shards_test = num_shards_test\n",
    "\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "        # Get lists of images and labels.\n",
    "        self.filenames, self.labels = \\\n",
    "            self.process_image_labels(labels_csv, label_file)\n",
    "\n",
    "        # Counter for total number of images processed.\n",
    "        self.counter = 0\n",
    "\n",
    "    def process_image_labels(self, labels_csv, label_file):\n",
    "        \"\"\"This will constuct two shuffled lists for images and labels.\n",
    "\n",
    "        The index of each image in the images list will have the corresponding\n",
    "        label at the same index in the labels list.\n",
    "        \"\"\"\n",
    "        labels_csv = io.open(labels_csv, 'r', encoding='utf-8')\n",
    "        labels_file = io.open(label_file, 'r',\n",
    "                              encoding='utf-8').read().splitlines()\n",
    "\n",
    "        # Map characters to indices.\n",
    "        label_dict = {}\n",
    "        count = 0\n",
    "        for label in labels_file:\n",
    "            label_dict[label] = count\n",
    "            count += 1\n",
    "\n",
    "        # Build the lists.\n",
    "        images = []\n",
    "        labels = []\n",
    "        for row in labels_csv:\n",
    "            file, label = row.strip().split(',')\n",
    "            images.append(file)\n",
    "            labels.append(label_dict[label])\n",
    "\n",
    "        # Randomize the order of all the images/labels.\n",
    "        shuffled_indices = list(range(len(images)))\n",
    "        random.seed(12121)\n",
    "        random.shuffle(shuffled_indices)\n",
    "        filenames = [images[i] for i in shuffled_indices]\n",
    "        labels = [labels[i] for i in shuffled_indices]\n",
    "\n",
    "        return filenames, labels\n",
    "\n",
    "    def write_tfrecords_file(self, output_path, indices):\n",
    "        \"\"\"Writes out TFRecords file.\"\"\"\n",
    "        writer = tf.python_io.TFRecordWriter(output_path)\n",
    "        for i in indices:\n",
    "            filename = self.filenames[i]\n",
    "            label = self.labels[i]\n",
    "            with tf.gfile.GFile(filename, 'rb') as f:\n",
    "                im_data = f.read()\n",
    "\n",
    "            # Example is a data format that contains a key-value store, where\n",
    "            # each key maps to a Feature message. In this case, each Example\n",
    "            # contains two features. One will be a ByteList for the raw image\n",
    "            # data and the other will be an Int64List containing the index of\n",
    "            # the corresponding label in the labels list from the file.\n",
    "            example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                'image/class/label': _int64_feature(label),\n",
    "                'image/encoded': _bytes_feature(tf.compat.as_bytes(im_data))}))\n",
    "            writer.write(example.SerializeToString())\n",
    "            self.counter += 1\n",
    "            if not self.counter % 1000:\n",
    "                print('Processed {} images...'.format(self.counter))\n",
    "        writer.close()\n",
    "\n",
    "    def convert(self):\n",
    "        \"\"\"This function will drive the conversion to TFRecords.\n",
    "\n",
    "        Here, we partition the data into a training and testing set, then\n",
    "        divide each data set into the specified number of TFRecords shards.\n",
    "        \"\"\"\n",
    "\n",
    "        num_files_total = len(self.filenames)\n",
    "\n",
    "        # Allocate about 15 percent of images to testing\n",
    "        num_files_test = int(num_files_total * .15)\n",
    "\n",
    "        # About 85 percent will be for training.\n",
    "        num_files_train = num_files_total - num_files_test\n",
    "\n",
    "        print('Processing training set TFRecords...')\n",
    "\n",
    "        files_per_shard = int(math.ceil(num_files_train /\n",
    "                                        self.num_shards_train))\n",
    "        start = 0\n",
    "        for i in range(1, self.num_shards_train):\n",
    "            shard_path = os.path.join(self.output_dir,\n",
    "                                      'train-{}.tfrecords'.format(str(i)))\n",
    "            # Get a subset of indices to get only a subset of images/labels for\n",
    "            # the current shard file.\n",
    "            file_indices = np.arange(start, start+files_per_shard, dtype=int)\n",
    "            start = start + files_per_shard\n",
    "            self.write_tfrecords_file(shard_path, file_indices)\n",
    "\n",
    "        # The remaining images will go in the final shard.\n",
    "        file_indices = np.arange(start, num_files_train, dtype=int)\n",
    "        final_shard_path = os.path.join(self.output_dir,\n",
    "                                        'train-{}.tfrecords'.format(\n",
    "                                            str(self.num_shards_train)))\n",
    "        self.write_tfrecords_file(final_shard_path, file_indices)\n",
    "\n",
    "        print('Processing testing set TFRecords...')\n",
    "\n",
    "        files_per_shard = math.ceil(num_files_test / self.num_shards_test)\n",
    "        start = num_files_train\n",
    "        for i in range(1, self.num_shards_test):\n",
    "            shard_path = os.path.join(self.output_dir,\n",
    "                                      'test-{}.tfrecords'.format(str(i)))\n",
    "            file_indices = np.arange(start, start+files_per_shard, dtype=int)\n",
    "            start = start + files_per_shard\n",
    "            self.write_tfrecords_file(shard_path, file_indices)\n",
    "\n",
    "        # The remaining images will go in the final shard.\n",
    "        file_indices = np.arange(start, num_files_total, dtype=int)\n",
    "        final_shard_path = os.path.join(self.output_dir,\n",
    "                                        'test-{}.tfrecords'.format(\n",
    "                                            str(self.num_shards_test)))\n",
    "        self.write_tfrecords_file(final_shard_path, file_indices)\n",
    "\n",
    "        print('\\nProcessed {} total images...'.format(self.counter))\n",
    "        print('Number of training examples: {}'.format(num_files_train))\n",
    "        print('Number of testing examples: {}'.format(num_files_test))\n",
    "        print('TFRecords files saved to {}'.format(self.output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_csv './image-data/labels-map.csv'\n",
    "labels_csv = DEFAULT_LABEL_CSV \n",
    "\n",
    "# label_file = './labels/2350-common-hangul.txt'\n",
    "label_file = DEFAULT_LABEL_FILE \n",
    "\n",
    "# output_dir './tfrecords-output' \n",
    "output_dir = DEFAULT_OUTPUT_DIR\n",
    "\n",
    "# num_shards_train 3\n",
    "num_shards_train = DEFAULT_NUM_SHARDS_TRAIN\n",
    "\n",
    "# num_shards_test 1\n",
    "num_shards_test = DEFAULT_NUM_SHARDS_TEST\n",
    "\n",
    "converter = TFRecordsConverter(labels_csv,\n",
    "                               label_file,\n",
    "                               output_dir,\n",
    "                               num_shards_train,\n",
    "                               num_shards_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training set TFRecords...\n",
      "Processed 1000 images...\n",
      "Processed 2000 images...\n",
      "Processed 3000 images...\n",
      "Processed 4000 images...\n",
      "Processed 5000 images...\n",
      "Processed 6000 images...\n",
      "Processed 7000 images...\n",
      "Processed 8000 images...\n",
      "Processed 9000 images...\n",
      "Processed 10000 images...\n",
      "Processed 11000 images...\n",
      "Processed 12000 images...\n",
      "Processed 13000 images...\n",
      "Processed 14000 images...\n",
      "Processed 15000 images...\n",
      "Processed 16000 images...\n",
      "Processed 17000 images...\n",
      "Processed 18000 images...\n",
      "Processed 19000 images...\n",
      "Processed 20000 images...\n",
      "Processed 21000 images...\n",
      "Processed 22000 images...\n",
      "Processed 23000 images...\n",
      "Processed 24000 images...\n",
      "Processed 25000 images...\n",
      "Processed 26000 images...\n",
      "Processed 27000 images...\n",
      "Processed 28000 images...\n",
      "Processed 29000 images...\n",
      "Processed 30000 images...\n",
      "Processed 31000 images...\n",
      "Processed 32000 images...\n",
      "Processed 33000 images...\n",
      "Processed 34000 images...\n",
      "Processed 35000 images...\n",
      "Processed 36000 images...\n",
      "Processed 37000 images...\n",
      "Processed 38000 images...\n",
      "Processed 39000 images...\n",
      "Processed 40000 images...\n",
      "Processed 41000 images...\n",
      "Processed 42000 images...\n",
      "Processed 43000 images...\n",
      "Processed 44000 images...\n",
      "Processed 45000 images...\n",
      "Processed 46000 images...\n",
      "Processed 47000 images...\n",
      "Processed 48000 images...\n",
      "Processed 49000 images...\n",
      "Processed 50000 images...\n",
      "Processed 51000 images...\n",
      "Processed 52000 images...\n",
      "Processed 53000 images...\n",
      "Processed 54000 images...\n",
      "Processed 55000 images...\n",
      "Processing testing set TFRecords...\n",
      "Processed 56000 images...\n",
      "Processed 57000 images...\n",
      "Processed 58000 images...\n",
      "Processed 59000 images...\n",
      "Processed 60000 images...\n",
      "Processed 61000 images...\n",
      "Processed 62000 images...\n",
      "Processed 63000 images...\n",
      "Processed 64000 images...\n",
      "Processed 65000 images...\n",
      "\n",
      "Processed 65800 total images...\n",
      "Number of training examples: 55930\n",
      "Number of testing examples: 9870\n",
      "TFRecords files saved to /home/work/5_korean/hangul-WR/./tfrecords-output\n"
     ]
    }
   ],
   "source": [
    "converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 1.14 on Python 3.6 (CUDA 9.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
